{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EcoTravel Agent - Configuração e Teste do Sistema RAG\n",
    "\n",
    "Este notebook demonstra a configuração detalhada do sistema RAG (Retrieval-Augmented Generation) com estratégias avançadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "# Configurar path para imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Tentar importar dependências RAG\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import faiss\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    RAG_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"Algumas dependências RAG não estão disponíveis: {e}\")\n",
    "    print(\"Execute: pip install sentence-transformers scikit-learn faiss-cpu rank-bm25\")\n",
    "    RAG_AVAILABLE = False\n",
    "\n",
    "# Configurar visualizações\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuração do Sistema RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAG_AVAILABLE:\n",
    "    from rag.rag_system import AdvancedRAGSystem\n",
    "    \n",
    "    # Inicializar sistema RAG\n",
    "    print(\"Inicializando sistema RAG...\")\n",
    "    rag = AdvancedRAGSystem(\n",
    "        data_path=\"../data\",\n",
    "        embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50,\n",
    "        top_k=10\n",
    "    )\n",
    "    \n",
    "    print(\"Sistema RAG inicializado com sucesso!\")\n",
    "    print(f\"Modelo de embedding: {rag.embedding_model_name}\")\n",
    "    print(f\"Tamanho do chunk: {rag.chunk_size}\")\n",
    "    print(f\"Sobreposição: {rag.chunk_overlap}\")\n",
    "else:\n",
    "    print(\"Sistema RAG não disponível. Instale as dependências necessárias.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Processamento de Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAG_AVAILABLE:\n",
    "    # Carregar documentos\n",
    "    print(\"Carregando documentos...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    documents = rag.load_documents()\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"Documentos carregados em {load_time:.2f} segundos\")\n",
    "    print(f\"Total de documentos: {len(documents)}\")\n",
    "    \n",
    "    # Analisar documentos carregados\n",
    "    doc_info = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_info.append({\n",
    "            \"index\": i,\n",
    "            \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "            \"type\": doc.metadata.get(\"type\", \"unknown\"),\n",
    "            \"length\": len(doc.page_content),\n",
    "            \"preview\": doc.page_content[:100] + \"...\"\n",
    "        })\n",
    "    \n",
    "    doc_df = pd.DataFrame(doc_info)\n",
    "    print(\"\\nInformações dos documentos carregados:\")\n",
    "    print(doc_df[['source', 'type', 'length']].to_string())\n",
    "else:\n",
    "    print(\"Carregamento de documentos não disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Criação de Chunks e Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAG_AVAILABLE:\n",
    "    # Criar chunks\n",
    "    print(\"Criando chunks semânticos...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    chunks = rag.create_chunks()\n",
    "    \n",
    "    chunk_time = time.time() - start_time\n",
    "    print(f\"Chunks criados em {chunk_time:.2f} segundos\")\n",
    "    print(f\"Total de chunks: {len(chunks)}\")\n",
    "    \n",
    "    # Analisar distribuição de tamanhos dos chunks\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "    \n",
    "    print(f\"\\nEstatísticas dos chunks:\")\n",
    "    print(f\"Tamanho médio: {np.mean(chunk_lengths):.1f} caracteres\")\n",
    "    print(f\"Tamanho mínimo: {np.min(chunk_lengths)} caracteres\")\n",
    "    print(f\"Tamanho máximo: {np.max(chunk_lengths)} caracteres\")\n",
    "    print(f\"Desvio padrão: {np.std(chunk_lengths):.1f} caracteres\")\n",
    "    \n",
    "    # Visualizar distribuição\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(chunk_lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribuição do Tamanho dos Chunks')\n",
    "    plt.xlabel('Tamanho do Chunk (caracteres)')\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.axvline(np.mean(chunk_lengths), color='red', linestyle='--', label=f'Média: {np.mean(chunk_lengths):.0f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(chunk_lengths)\n",
    "    plt.title('Box Plot do Tamanho dos Chunks')\n",
    "    plt.ylabel('Tamanho do Chunk (caracteres)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar exemplos de chunks\n",
    "    print(\"\\nExemplos de chunks criados:\")\n",
    "    for i in range(min(3, len(chunks))):\n",
    "        print(f\"\\nChunk {i+1}:\")\n",
    "        print(f\"Tamanho: {len(chunks[i].page_content)} caracteres\")\n",
    "        print(f\"Fonte: {chunks[i].metadata.get('source', 'unknown')}\")\n",
    "        print(f\"Conteúdo: {chunks[i].page_content[:200]}...\")\n",
    "else:\n",
    "    print(\"Criação de chunks não disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Criação de Embeddings e Índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAG_AVAILABLE:\n",
    "    # Criar embeddings\n",
    "    print(\"Criando embeddings...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    embeddings = rag.create_embeddings()\n",
    "    \n",
    "    embedding_time = time.time() - start_time\n",
    "    print(f\"Embeddings criados em {embedding_time:.2f} segundos\")\n",
    "    print(f\"Shape dos embeddings: {embeddings.shape}\")\n",
    "    print(f\"Dimensão dos embeddings: {embeddings.shape[1]}\")\n",
    "    \n",
    "    # Construir índice FAISS\n",
    "    print(\"\\nConstruindo índice FAISS...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    rag.build_faiss_index()\n",
    "    \n",
    "    faiss_time = time.time() - start_time\n",
    "    print(f\"Índice FAISS construído em {faiss_time:.2f} segundos\")\n",
    "    print(f\"Total de vetores no índice: {rag.faiss_index.ntotal}\")\n",
    "    \n",
    "    # Construir índice BM25\n",
    "    print(\"\\nConstruindo índice BM25...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    rag.build_bm25_index()\n",
    "    \n",
    "    bm25_time = time.time() - start_time\n",
    "    print(f\"Índice BM25 construído em {bm25_time:.2f} segundos\")\n",
    "    \n",
    "    # Resumo dos tempos\n",
    "    total_time = load_time + chunk_time + embedding_time + faiss_time + bm25_time\n",
    "    print(f\"\\n=== Resumo dos Tempos de Construção ===\")\n",
    "    print(f\"Carregamento: {load_time:.2f}s ({load_time/total_time*100:.1f}%)\")\n",
    "    print(f\"Chunking: {chunk_time:.2f}s ({chunk_time/total_time*100:.1f}%)\")\n",
    "    print(f\"Embeddings: {embedding_time:.2f}s ({embedding_time/total_time*100:.1f}%)\")\n",
    "    print(f\"FAISS: {faiss_time:.2f}s ({faiss_time/total_time*100:.1f}%)\")\n",
    "    print(f\"BM25: {bm25_time:.2f}s ({bm25_time/total_time*100:.1f}%)\")\n",
    "    print(f\"Total: {total_time:.2f}s\")\n",
    "else:\n",
    "    print(\"Criação de embeddings não disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Teste de Diferentes Estratégias de Busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAG_AVAILABLE:\n",
    "    # Definir consultas de teste\n",
    "    test_queries = [\n",
    "        \"Como viajar de São Paulo para Rio de Janeiro de forma sustentável?\",\n",
    "        \"Qual o transporte com menor emissão de carbono?\",\n",
    "        \"Hotéis sustentáveis no Rio de Janeiro\",\n",
    "        \"Atividades ecológicas e ecoturismo\",\n",
    "        \"Compensação de carbono para viagens\"\n",
    "    ]\n",
    "    \n",
    "    # Testar diferentes métodos de busca\n",
    "    methods = [\"semantic\", \"lexical\", \"hybrid\"]\n",
    "    \n",
    "    search_results = {}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CONSULTA: {query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        search_results[query] = {}\n",
    "        \n",
    "        for method in methods:\n",
    "            start_time = time.time()\n",
    "            results = rag.search(query, method=method, k=3, rerank=True)\n",
    "            search_time = time.time() - start_time\n",
    "            \n",
    "            search_results[query][method] = {\n",
    "                \"results\": results,\n",
    "                \"time\": search_time\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n--- Método: {method.upper()} ({search_time:.3f}s) ---\")\n",
    "            \n",
    "            for i, result in enumerate(results[:2], 1):  # Mostrar top 2\n",
    "                score = result.get('rerank_score', result.get('score', 0))\n",
    "                print(f\"{i}. Score: {score:.3f}\")\n",
    "                print(f\"   Conteúdo: {result['content'][:150]}...\")\n",
    "                print(f\"   Fonte: {result['metadata'].get('source', 'unknown')}\")\n",
    "else:\n",
    "    print(\"Teste de busca não disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análise de Performance dos Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAG_AVAILABLE and 'search_results' in locals():\n",
    "    # Análise de tempos de busca\n",
    "    time_analysis = []\n",
    "    \n",
    "    for query, methods_data in search_results.items():\n",
    "        for method, data in methods_data.items():\n",
    "            time_analysis.append({\n",
    "                \"query\": query[:30] + \"...\",\n",
    "                \"method\": method,\n",
    "                \"time\": data[\"time\"],\n",
    "                \"num_results\": len(data[\"results\"])\n",
    "            })\n",
    "    \n",
    "    time_df = pd.DataFrame(time_analysis)\n",
    "    \n",
    "    # Visualizar tempos de busca\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Tempo médio por método\n",
    "    plt.subplot(1, 3, 1)\n",
    "    avg_times = time_df.groupby('method')['time'].mean()\n",
    "    bars = plt.bar(avg_times.index, avg_times.values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    plt.title('Tempo Médio de Busca por Método')\n",
    "    plt.ylabel('Tempo (segundos)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                 f'{height:.3f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # Box plot dos tempos\n",
    "    plt.subplot(1, 3, 2)\n",
    "    time_df.boxplot(column='time', by='method', ax=plt.gca())\n",
    "    plt.title('Distribuição dos Tempos de Busca')\n",
    "    plt.suptitle('')  # Remove título automático\n",
    "    \n",
    "    # Heatmap de tempos por query e método\n",
    "    plt.subplot(1, 3, 3)\n",
    "    pivot_times = time_df.pivot(index='query', columns='method', values='time')\n",
    "    sns.heatmap(pivot_times, annot=True, fmt='.3f', cmap='YlOrRd')\n",
    "    plt.title('Tempos de Busca por Query e Método')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estatísticas detalhadas\n",
    "    print(\"\\n=== Estatísticas de Performance ===\")\n",
    "    print(time_df.groupby('method')['time'].agg(['mean', 'std', 'min', 'max']).round(4))\n",
    "else:\n",
    "    print(\"Análise de performance não disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análise de Qualidade dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAG_AVAILABLE and 'search_results' in locals():\n",
    "    # Análise de diversidade de fontes\n",
    "    source_analysis = []\n",
    "    \n",
    "    for query, methods_data in search_results.items():\n",
    "        for method, data in methods_data.items():\n",
    "            sources = set()\n",
    "            scores = []\n",
    "            \n",
    "            for result in data[\"results\"]:\n",
    "                source = result['metadata'].get('source', 'unknown')\n",
    "                sources.add(source)\n",
    "                score = result.get('rerank_score', result.get('score', 0))\n",
    "                scores.append(score)\n",
    "            \n",
    "            source_analysis.append({\n",
    "                \"query\": query[:30] + \"...\",\n",
    "                \"method\": method,\n",
    "                \"unique_sources\": len(sources),\n",
    "                \"avg_score\": np.mean(scores) if scores else 0,\n",
    "                \"max_score\": np.max(scores) if scores else 0,\n",
    "                \"score_std\": np.std(scores) if scores else 0\n",
    "            })\n",
    "    \n",
    "    source_df = pd.DataFrame(source_analysis)\n",
    "    \n",
    "    # Visualizar análise de qualidade\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Diversidade de fontes\n",
    "    plt.subplot(1, 3, 1)\n",
    "    avg_sources = source_df.groupby('method')['unique_sources'].mean()\n",
    "    bars = plt.bar(avg_sources.index, avg_sources.values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    plt.title('Diversidade Média de Fontes')\n",
    "    plt.ylabel('Número de Fontes Únicas')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                 f'{height:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Score médio\n",
    "    plt.subplot(1, 3, 2)\n",
    "    avg_scores = source_df.groupby('method')['avg_score'].mean()\n",
    "    bars = plt.bar(avg_scores.index, avg_scores.values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    plt.title('Score Médio dos Resultados')\n",
    "    plt.ylabel('Score Médio')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Consistência (desvio padrão dos scores)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    avg_std = source_df.groupby('method')['score_std'].mean()\n",
    "    bars = plt.bar(avg_std.index, avg_std.values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    plt.title('Consistência dos Scores\\n(menor = mais consistente)')\n",
    "    plt.ylabel('Desvio Padrão dos Scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                 f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== Análise de Qualidade dos Resultados ===\")\n",
    "    quality_summary = source_df.groupby('method').agg({\n",
    "        'unique_sources': ['mean', 'std'],\n",
    "        'avg_score': ['mean', 'std'],\n",
    "        'max_score': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    print(quality_summary)\n",
    "else:\n",
    "    print(\"Análise de qualidade não disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exemplo de Contexto Gerado para LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAG_AVAILABLE:\n",
    "    # Demonstrar geração de contexto\n",
    "    example_query = \"Quero viajar de São Paulo para Salvador de forma sustentável. Quais são as melhores opções?\"\n",
    "    \n",
    "    print(f\"CONSULTA: {example_query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Gerar contexto usando o método híbrido\n",
    "    context = rag.get_context_for_query(example_query, max_context_length=1500)\n",
    "    \n",
    "    print(\"CONTEXTO GERADO PELO RAG:\")\n",
    "    print(\"-\"*40)\n",
    "    print(context)\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Tamanho do contexto: {len(context)} caracteres\")\n",
    "    \n",
    "    # Simular prompt completo para o LLM\n",
    "    system_prompt = \"\"\"\n",
    "Você é um assistente especializado em viagens sustentáveis. \n",
    "Use as informações do contexto para responder de forma precisa e útil.\n",
    "\n",
    "CONTEXTO:\n",
    "{context}\n",
    "\n",
    "PERGUNTA: {query}\n",
    "\n",
    "RESPOSTA:\"\"\"\n",
    "    \n",
    "    full_prompt = system_prompt.format(context=context, query=example_query)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROMPT COMPLETO PARA O LLM:\")\n",
    "    print(\"=\"*80)\n",
    "    print(full_prompt)\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Tamanho total do prompt: {len(full_prompt)} caracteres\")\n",
    "else:\n",
    "    print(\"Geração de contexto não disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Benchmark e Métricas do Sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAG_AVAILABLE:\n",
    "    # Criar benchmark de consultas\n",
    "    benchmark_queries = [\n",
    "        \"transporte sustentável\",\n",
    "        \"hotéis ecológicos\",\n",
    "        \"emissão carbono avião\",\n",
    "        \"ônibus versus trem\",\n",
    "        \"atividades locais\",\n",
    "        \"compensação carbono\",\n",
    "        \"energia solar hotel\",\n",
    "        \"mercados locais\",\n",
    "        \"certificação LEED\",\n",
    "        \"ecoturismo Brasil\"\n",
    "    ]\n",
    "    \n",
    "    # Executar benchmark\n",
    "    benchmark_results = []\n",
    "    \n",
    "    print(\"Executando benchmark do sistema RAG...\")\n",
    "    \n",
    "    for i, query in enumerate(benchmark_queries):\n",
    "        print(f\"Processando query {i+1}/{len(benchmark_queries)}: {query}\")\n",
    "        \n",
    "        # Testar busca híbrida\n",
    "        start_time = time.time()\n",
    "        results = rag.search(query, method=\"hybrid\", k=5, rerank=True)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        # Calcular métricas\n",
    "        if results:\n",
    "            avg_score = np.mean([r.get('rerank_score', r.get('score', 0)) for r in results])\n",
    "            max_score = np.max([r.get('rerank_score', r.get('score', 0)) for r in results])\n",
    "            unique_sources = len(set([r['metadata'].get('source', 'unknown') for r in results]))\n",
    "        else:\n",
    "            avg_score = max_score = unique_sources = 0\n",
    "        \n",
    "        benchmark_results.append({\n",
    "            \"query\": query,\n",
    "            \"search_time\": search_time,\n",
    "            \"num_results\": len(results),\n",
    "            \"avg_score\": avg_score,\n",
    "            \"max_score\": max_score,\n",
    "            \"unique_sources\": unique_sources\n",
    "        })\n",
    "    \n",
    "    benchmark_df = pd.DataFrame(benchmark_results)\n",
    "    \n",
    "    # Estatísticas do benchmark\n",
    "    print(\"\\n=== RESULTADOS DO BENCHMARK ===\")\n",
    "    print(f\"Tempo médio de busca: {benchmark_df['search_time'].mean():.3f}s\")\n",
    "    print(f\"Tempo total: {benchmark_df['search_time'].sum():.3f}s\")\n",
    "    print(f\"Score médio: {benchmark_df['avg_score'].mean():.3f}\")\n",
    "    print(f\"Fontes únicas médias: {benchmark_df['unique_sources'].mean():.1f}\")\n",
    "    print(f\"Resultados por query: {benchmark_df['num_results'].mean():.1f}\")\n",
    "    \n",
    "    # Visualizar benchmark\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Tempo de busca por query\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.bar(range(len(benchmark_queries)), benchmark_df['search_time'], alpha=0.7)\n",
    "    plt.title('Tempo de Busca por Query')\n",
    "    plt.xlabel('Query Index')\n",
    "    plt.ylabel('Tempo (s)')\n",
    "    plt.xticks(range(len(benchmark_queries)), [f\"Q{i+1}\" for i in range(len(benchmark_queries))])\n",
    "    \n",
    "    # Score médio por query\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(range(len(benchmark_queries)), benchmark_df['avg_score'], alpha=0.7, color='green')\n",
    "    plt.title('Score Médio por Query')\n",
    "    plt.xlabel('Query Index')\n",
    "    plt.ylabel('Score Médio')\n",
    "    plt.xticks(range(len(benchmark_queries)), [f\"Q{i+1}\" for i in range(len(benchmark_queries))])\n",
    "    \n",
    "    # Diversidade de fontes\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(range(len(benchmark_queries)), benchmark_df['unique_sources'], alpha=0.7, color='orange')\n",
    "    plt.title('Diversidade de Fontes por Query')\n",
    "    plt.xlabel('Query Index')\n",
    "    plt.ylabel('Fontes Únicas')\n",
    "    plt.xticks(range(len(benchmark_queries)), [f\"Q{i+1}\" for i in range(len(benchmark_queries))])\n",
    "    \n",
    "    # Distribuição dos tempos\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(benchmark_df['search_time'], bins=10, alpha=0.7, color='purple')\n",
    "    plt.title('Distribuição dos Tempos de Busca')\n",
    "    plt.xlabel('Tempo (s)')\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.axvline(benchmark_df['search_time'].mean(), color='red', linestyle='--', \n",
    "                label=f'Média: {benchmark_df[\"search_time\"].mean():.3f}s')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabela detalhada\n",
    "    print(\"\\nResultados detalhados do benchmark:\")\n",
    "    print(benchmark_df.round(3).to_string(index=False))\n",
    "else:\n",
    "    print(\"Benchmark não disponível.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumo e Conclusões\n",
    "\n",
    "### Principais Achados:\n",
    "\n",
    "1. **Performance**: O sistema RAG demonstra tempos de busca consistentes e eficientes\n",
    "2. **Qualidade**: A busca híbrida combina vantagens semânticas e lexicais\n",
    "3. **Diversidade**: Resultados abrangem diferentes fontes de conhecimento\n",
    "4. **Escalabilidade**: Sistema otimizado para base de conhecimento sustentável\n",
    "\n",
    "### Próximos Passos:\n",
    "- Integração com LLM para geração de respostas\n",
    "- Implementação de feedback loop para melhoria contínua\n",
    "- Expansão da base de conhecimento\n",
    "- Otimização de parâmetros baseada em métricas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}